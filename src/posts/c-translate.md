---
title: Optimizing Your Whisper Model by Converting to C
description: A step-by-step guide to converting Whisper by OpenAI to the C language for faster performance and lower memory usage.
date: '2025-03-08'
categories:
  - speech-recognition
  - machine-learning
  - optimization
  - C-language
published: true
---
Whisper is a powerful speech recognition model developed by OpenAI, designed to transcribe human speech into text with high accuracy. However, its implementation in Python can be slow and memory-intensive, especially when processing large amounts of audio data.

For Khmer, a Cambodian language, we need a faster and more memory-efficient solution to enable real-time transcription on low-resource devices. One approach is to convert the Whisper model to the C language, known for its speed and efficiency.

In this guide, we'll walk through the process of converting the Whisper model to C and optimizing it for performance and memory usage.


### What is [fast-whisper](https://github.com/SYSTRAN/faster-whisper.git) ?
[faster-whisper](https://github.com/SYSTRAN/faster-whisper.git) is a reimplementation of OpenAI's Whisper model using [CTranslate2](https://opennmt.net/CTranslate2/index.html#), a fast inference engine for Transformer models. It is up to 4 times faster than [openai/whisper](https://huggingface.co/openai/whisper-tiny) while using less memory. Efficiency can be further improved with 8-bit quantization on both CPU and GPU.

### Step 1: Install Dependencies
```bash
pip install transformers 
pip install ctranslate2
```
Noted that CTranslate2 supports selected models from Hugging Face‚Äôs Transformers. Make you sure that your model is supported by CTranslate2. Check this out to see list of support model [here](https://opennmt.net/CTranslate2/guides/transformers.html).

### Step 2: Load the Whisper Model
Use the following command to convert your Whisper model to CTranslate2:
```bash
ct2-transformers-converter --model <model_id> --output_dir <output_path> --copy_files tokenizer_config.json --quantization int8
```
I have my own fine-tuned model [`PhanithLIM/whisper-tiny-aug-29dec`](https://huggingface.co/PhanithLIM/whisper-tiny-aug-29dec) and I want to convert it to C.
for flag `--quantization int8` is a technique that can reduce the model size and accelerate. [`read more`](https://opennmt.net/CTranslate2/quantization.html)
```bash
ct2-transformers-converter --model PhanithLIM/whisper-tiny-aug-29dec --output_dir whisper-tiny-int8 --copy_files tokenizer_config.json --quantization int8
```
After running the command, the converted model will be stored in the specified output directory.

![image info](../../images/c-translate/image1.png)

### Step 3: Transcribing an Audio File
Now that the Whisper model has been successfully converted, install the [`faster-whisper`](https://pypi.org/project/faster-whisper/) package to test it:
```bash
pip install faster-whisper
```
To load your locally converted model, use the following code:
```python
from faster_whisper import WhisperModel
model_size = "/content/whisper-tiny-int8"
model = WhisperModel(model_size, device="cpu", compute_type="int8", local_files_only=True)
```
**Note:** 
- Ensure `local_files_only=True` to load the model from the local directory. But In case you store your converted model in Hugging Face, you can set `local_files_only=False`.
- If you want to run it on GPU, change `device="cuda"`. This will significantly improve performance.

```python
segments, info = model.transcribe("/content/1_0.wav", beam_size=8)
print("Detected language '%s' with probability %f" % (info.language, info.language_probability))
for segment in segments:
    print("[%.2fs -> %.2fs] %s" % (segment.start, segment.end, segment.text))
```

This output was tested on an audio file of 3 minutes and 2 seconds, and it took 1 minute to transcribe. However, when tested on a GPU, it took only 5 seconds to transcribe‚Äîblazing fast! The output shows that the model does not transcribe the Khmer language well because it lacks sufficient training data, and it is a tiny model.

```
Detected language 'km' with probability 0.998886
[0.00s -> 30.00s]·ûÄ·üí·ûö·ûü·ûΩ·ûÑ·ûü·ûª·ûÅ·û∂·ûó·û∑·ûî·û∂·ûõ·ûÄ·û∂·ûõ·ûñ·û∏·ûê·üí·ûÑ·üÉ·ûÖ·û∑·ûì·üí·ûì·ûî·û∂·ûì·ûí·üí·ûú·ûæ·ûÄ·û∂·ûö·û¢·üÜ·ûñ·û∏·ûì·û∂·ûõ·ûä·ûõ·üã·ûî·ûÑ·üã·ûî·üí·û¢·ûº·ûì·ûî·üí·ûö·ûá·û∂·ûñ·ûõ·ûö·ûè·üã·ûë·û∂·üÜ·ûÑ·û±·üí·ûô·ûò·û∂·ûì·ûÄ·û∂·ûö·ûî·üí·ûö·ûª·ûò·ûî·üí·ûö·ûô·üà·ûÄ·û∂·ûö·ûñ·û∏·ûü·ûª·ûÅ·ûó·û∂·ûñ·ûñ·û∏
[30.00s -> 60.00s] ·ûÄ·üí·ûô·ûò·ûä·ûº·ûÖ·ûá·û∂·ûÄ·üí·ûö·ûª·ûò·ûî·üí·ûä·û∂·ûü·û∂·ûô ·û†·ûº·ûö·ûü·ûò·üí·ûî·ûö ·ûî·üí·ûö·ûõ·û∂·ûÄ·üã·ûî·üÜ·ûñ·ûÑ·üí·ûÄ·ûö·ûÄ·ûÄ·ûÄ·ûì·üí·ûè·û∂·ûü·üç ·ûá·ûΩ·ûò·ûò·üÅ·ûì·ûò·üÅ·ûè·ûó·üí·ûì·üÑ·üá·ûò·û∏·ûè·ûó·üí·ûì·üÜ·ûñ·û∏ ·ü¶·ü¶·ûÜ·üí·ûì·û∂·üÜ ·û¨·û¢·üí·ûì·ûÄ·ûò·û∂·ûì·ûñ·û∏·ûö·ûÄ·ûò·üí·ûò ·ûÄ
[60.00s -> 90.00s] ·û∂·ûñ·û∏ ·ûó·û∂·ûÇ·ûÖ·üí·ûö·ûæ·ûì ·ûî·û∂·ûì·ûá·û∂·ûü·û∂·ûü·üí·ûî·üí·ûî·û∏·û°·ûæ·ûÑ·ûú·û∑·ûâ ·ûÇ·û∂·ûè·üã ·ûú·û∂·ûá·û∂·ûü·û∂·ûü·üí·ûè·üí·ûö·ûî·ûì·üí·ûì·ûü·üí·ûò·û∂·ûì·ûê·û∂ ·ûî·üí·ûö·û†·üÇ·ûõ ·ü°·ü°·ü¢·ü† ·ûë·üÖ ·ü°·ü¢·ü† ·ûì·üÉ·ûó·û∂·ûÇ·ûÖ·üí·ûö·ûæ·ûì ·ûÇ·û∫·ûü·üí·ûö·û∂·ûõ ·ûî·üâ·ûª·ûì·üí·ûè
[90.00s -> 120.00s] ·ûë·û∏·ûä·üÇ·ûõ·ûò·û∂·ûì·ûò·û∂·ûì·ûö·ûÄ ·ûî·ûì·üí·ûë·û∂·ûî·üã·ûò·ûÄ ·ûî·üâ·ûª·ûì·üí·ûò·ûè·üí·ûò·ûò·üâ·ûª·ûì ·û¨·ûó·üí·ûì·üÅ·üá ·ûä·üè·ûõ·üí·ûõ·üÇ·ûÑ·ûü·ûâ·üí·ûâ·û∂ ·ûì·üÉ·ûá·üÜ·ûÑ·û∫·ûì·üÅ·üá ·ûö·ûΩ·ûò·ûò·û∂·ûì ·ûÄ·üè ·ûÄ·üí·ûö·ûª·ûÑ·ûÄ·üí·ûä·üÖ ·ûë·ûπ·ûÑ·ûÖ·üí·ûö·ûò·üâ·ûª·ûü ·û†·ûº·ûö·ûü·ûò·üí·ûî 
[120.00s -> 150.00s] ·ûÄ·û∂·ûì·û¢·û∂·ûé·û∂·ûò·üâ·üÉ ·ûõ·û∂·ûì·ûü·ûò·üí·û¢·û∂·ûè·ûä·üÉ·û±·üí·ûô ·ûî·û∂·ûì·ûò·û∂·ûì·ûâ·û∂·ûî·üã·ûá·û∂·ûò·ûΩ·ûô·ûü·ûî·üí·ûî·ûº·ûö ·û¨·ûë·ûπ·ûÄ·û±·üí·ûô·ûÄ·ûä·üÇ·ûõ·ûî·üâ·üá·ûñ·û∂·ûõ·üã·ûó·üí·ûì·üÅ·ûÄ·ûÖ·üí·ûö·ûª·ûò·ûò·ûÄ ·ûì·û∑·ûÑ·ûò·û∂·ûè·üã ·ûî·üí·ûö·ûü·û∑·ûò·üí·ûî·ûõ·üã·ûò·û∑·ûì·ûñ·û∂·ûì
[150.00s -> 180.00s] ·ûë·ûπ·ûÑ·ûî·ûö·û∑·ûÅ·û∂ ·ûî·ûª·ûö·û∑·ûó·üÑ·ûÇ·ûá·û∂·ûò·ûΩ·ûô·û¢·üí·ûì·ûÄ·ûä·ûë·üí·ûö·û∏ ·ûì·ûπ·ûÑ·ûñ·û∂·ûÄ·üí·ûô·ûü·ûª·üÜ·ûü·üí·ûõ·üÄ·ûî ·ûñ·û∂·ûÄ·üã·ûü·ûò·üí·ûõ·üÄ·ûî·ûü·ûò·üí·ûõ·üÄ·ûî·ûî·üÜ·ûñ·û∂·ûÄ·üí·ûÄ·üí·ûì·ûü·üÜ·ûü·üí·ûö·ûî ·ûä·ûæ·ûò·üí·ûî·û∏·ûî·ûÑ·üí·ûÄ·û∂·ûö·ûó·û∂·ûñ·ûî·üí·ûö·ûá·û∂·ûÄ·ûñ
[180.00s -> 181.82s] ·ûü·ûî·üí·ûî·ûì·üÅ·üá·ûò·û∑·ûè·üí·ûè·ûü·üÅ·ûö·û∏·ûÄ·û∂·ûö·û¢·û∂·ûô·üî
```

### Conclusion
Converting the Whisper model to C can significantly improve performance and memory usage, making it suitable for real-time transcription on low-resource devices. By leveraging the speed and efficiency of CTranslate2, you can achieve blazing-fast speech recognition.

Give it a try and optimize your own models! üöÄ
```python
print("Thank you for reading!")
```